# Understanding Storage Levels and Data Flow through the Inductiva API
>some text here- Maya

## Overview
When talking about concepts like storage at Inductive, there are 3 levels you 
need to consider:

**Local Storage:** this is the storage of your local machine, from where you are 
typically writing Python scripts that call the Inductiva API. Inductiva does not 
have direct access to this storage. However, the API has primitives that allow you 
to exchange information from your local storage to your Personal Remote Storage 
(see below).

**Personal Remote Storage:** this is a folder that lives on Inductiva cloud storage 
that is exclusively dedicated to you, and can only be accessed by you. We use this 
space both to store input files that you submit via the API (typically when you 
call the run() method of a simulator object), and to share with you the potentially 
large output files generated by the simulators you invoke.

**Worker Storage:** this is the storage that is available on the worker VMs that pick 
up simulation tasks. This is used to store the input files required for the simulation 
to run as well as the files produced by the simulation. before these get transferred 
to your Personal Remote Storage. You don’t have direct access to this storage. All 
communication to your Local Storage needs to be done via your Personal Remote Storage folder.

So, what is the typical flow of that when you invoke a remote simulator using the 
Inductiva API? 

Let’s start by assuming that somewhere in your local storage, typically in a 
folder dedicated to your project, you have several files that are required for 
running the simulation. Usually this data includes one or more files describing 
the simulation case, and file specifying how the simulator should be parameterized, 
as well as files describes assets (e.g. 3D shapes, information about chemical compounds, 
bathymetric profiles, etc) that will be used in or that are themselves the target 
of the simulation. All this data is input to the simulator software and, of course, 
will have to be sent to Inductiva machines where the simulator is going to execute. 
So, the first step in the data flow is to upload all these files to our server. 
This upload is triggered when you call the run method of the simulator object 
you are using. 

Here is an example. Let us assume you are developing a coastal dynamics study 
using Reef3D, and you have all the required input files and assets stored in the 
subdirectory “my_input_data_dir” located inside your project folder on your local 
machine. The following piece of code illustrates this situation:
``````
code
``````

# Initialize the simulator object
simulator = inductiva.simulators.REEF3D()

# Invoke the run() method of the simulator object. 

# This will trigger the packing and uploading the data
task = simulator.run(input_dir=my_input_data_dir)
…

The moment you invoke run() you start the uploading process. The folder my_input_data_dir 
is zipped and the corresponding zip file is uploaded to Inductiva servers.
 
You can check what is actually happening when you invoke a simulator via the API. 
If you look at the logs produced at you will be able to see a message like this 
right in the beginning of the process execution:

```bash
Task Information:
> ID:                    tc7cwuer45kfzuw8t93r6dxa8
> Method:                swash
> Local input directory: swash-resources-example
> Submitting to the following computational resources:
 >> Default queue with c2-standard-4 machines.
Preparing upload of the local input directory swash-resources-example (160 B).
Local input directory successfully uploaded.
Task tc7cwuer45kfzuw8t93r6dxa8 submitted to the default queue.
Simulation metadata logged to: inductiva_output/task_metadata.json
```

Once the zip file gets to the Inductiva server, it is immediately transferred to 
your Personal Remote Storage area, under a folder whose name is the id for the 
simulation task you invoked. You can check the contents of your  Personal Remote 
Storage programmatically via the API or by using the CLI. Next, we show how you 
would be able to check the uploaded zip file using the CLI.

>@ivan can you add a few CLI examples of how to check the personal area
To check your personal storage area, you can do a general listing of the contents with:
```bash
$ inductiva storage ls
Name                        Size      Creation Time
--------------------------  --------  ----------------
tc7cwuer45kfzuw8t93r6dxa8/  1.53 MB   01 Feb, 23:52:43
hzgk5ngzk28a39qa7mesv0snk/  1.53 MB   01 Feb, 23:45:17
mjnb8c7i8bfppgmu2y1zd1o7f/  11.52 MB  01 Feb, 23:24:17
57mr4kas99jxb9titkeackano/  11.52 MB  01 Feb, 23:07:17
ox8718m0pwfi02zczui3qky4w/  11.52 MB  01 Feb, 23:07:16
mak1ji62s7axf7mespkc36g7e/  11.52 MB  01 Feb, 23:07:14
ijyu8bkvme7vg9k0kj6v23gxa/  11.52 MB  01 Feb, 23:07:13
g5qq5c9mk2nr5wqhzef38sdm4/  11.52 MB  01 Feb, 23:07:11
fxobdn63z9xtb7q3thhpwn7c7/  11.52 MB  01 Feb, 22:53:10
jyc8b91mj556w9u61f8qrhf4b/  11.29 MB  01 Feb, 20:29:17
```

The simulation we have just invoked has the task ID `hzgk5ngzk28a39qa7mesv0snk` and we can check that its contents were correctly submitted to the server by listing the specific contents of the task folder with:
```bash
$ inductiva storage ls tc7cwuer45kfzuw8t93r6dxa8
Name       Size     Creation Time
---------  -------  ----------------
input.zip  1.53 MB  01 Feb, 23:52:44
           0 B      01 Feb, 23:52:43
```


>Luis will explain the rest when this first part is already on readthedocs. 
